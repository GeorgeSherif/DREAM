{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "ratings1 = [4,5,5,5,5,4,5,4,4,4,4,3,5,3,4,5]\n",
        "ratings2 = [5,5,5,5,5,5,4,5,5,4,4,5,5,5,5,5]\n",
        "\n",
        "# Calculate observed agreement po\n",
        "agreements = sum(np.array(ratings1) == np.array(ratings2))\n",
        "total = len(ratings1)\n",
        "p_o = agreements / total\n",
        "\n",
        "# Calculate expected agreement pe\n",
        "rater1_counts = Counter(ratings1)\n",
        "rater2_counts = Counter(ratings2)\n",
        "\n",
        "n = sum(rater1_counts.values())  # total ratings\n",
        "pe_categories = 0\n",
        "for category in set(ratings1 + ratings2):\n",
        "    pe_categories += (rater1_counts[category] / n) * (rater2_counts[category] / n)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Observed Agreement (p_o): {p_o:.2f}\")\n",
        "print(f\"Expected Agreement (p_e): {pe_categories:.2f}\")"
      ],
      "metadata": {
        "id": "Ugpsa6ISoTkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f25520-4679-45bc-9b42-17f8f565d6ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observed Agreement (p_o): 0.50\n",
            "Expected Agreement (p_e): 0.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbnMBoUhiPK8",
        "outputId": "bce40fc8-6873-4669-aacc-3477f96d7be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observed Agreement (p_o): 0.69\n",
            "Expected Agreement (p_e): 0.44\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "ratings1 = [4,5,5,4,5,5,5,5,5,5,3,4,5,4,5,4]\n",
        "ratings2 = [4,5,5,4,5,4,5,5,4,5,3,4,4,3,5,5]\n",
        "\n",
        "# Calculate observed agreement po\n",
        "agreements = sum(np.array(ratings1) == np.array(ratings2))\n",
        "total = len(ratings1)\n",
        "p_o = agreements / total\n",
        "\n",
        "# Calculate expected agreement pe\n",
        "rater1_counts = Counter(ratings1)\n",
        "rater2_counts = Counter(ratings2)\n",
        "\n",
        "n = sum(rater1_counts.values())  # total ratings\n",
        "pe_categories = 0\n",
        "for category in set(ratings1 + ratings2):\n",
        "    pe_categories += (rater1_counts[category] / n) * (rater2_counts[category] / n)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Observed Agreement (p_o): {p_o:.2f}\")\n",
        "print(f\"Expected Agreement (p_e): {pe_categories:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def calculate_cohens_kappa(ratings1, ratings2):\n",
        "    \"\"\"\n",
        "    Calculate Cohen's Kappa for two arrays of categorical ratings.\n",
        "\n",
        "    Args:\n",
        "    ratings1 (list or array): Ratings by Annotator 1.\n",
        "    ratings2 (list or array): Ratings by Annotator 2.\n",
        "\n",
        "    Returns:\n",
        "    float: Cohen's Kappa coefficient.\n",
        "    \"\"\"\n",
        "    kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    return kappa\n",
        "\n",
        "# Example usage:\n",
        "ratings1 = [4,5,5,5,5,4,5,4,4,4,4,3,5,3,4,5]\n",
        "ratings2 = [5,5,5,5,5,5,4,5,5,4,4,5,5,5,5,5]\n",
        "\n",
        "\n",
        "\n",
        "kappa = calculate_cohens_kappa(ratings1, ratings2)\n",
        "print(f\"Cohen's Kappa: {kappa:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgEShxCmzpbK",
        "outputId": "5c197bcd-2705-4558-af10-c758a8141ac0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa: 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def calculate_cohens_kappa(ratings1, ratings2):\n",
        "    \"\"\"\n",
        "    Calculate Cohen's Kappa for two arrays of categorical ratings.\n",
        "\n",
        "    Args:\n",
        "    ratings1 (list or array): Ratings by Annotator 1.\n",
        "    ratings2 (list or array): Ratings by Annotator 2.\n",
        "\n",
        "    Returns:\n",
        "    float: Cohen's Kappa coefficient.\n",
        "    \"\"\"\n",
        "    kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    return kappa\n",
        "\n",
        "# Example usage:\n",
        "ratings1 = [4,5,5,4,5,5,5,5,5,5,3,4,5,4,5,4]\n",
        "ratings2 = [4,5,5,4,5,4,5,5,4,5,3,4,4,3,5,5]\n",
        "\n",
        "\n",
        "\n",
        "kappa = calculate_cohens_kappa(ratings1, ratings2)\n",
        "print(f\"Cohen's Kappa: {kappa:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0LOEzKXzjt7",
        "outputId": "faa469e0-1ee7-4112-ff89-a7988fd458e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa: 0.44\n"
          ]
        }
      ]
    }
  ]
}