{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Authenticity - 5 classes\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "ratings1 = [4,5,5,5,5,4,5,4,4,4,4,3,5,3,4,5]\n",
        "ratings2 = [5,5,5,5,5,5,4,5,5,4,4,5,5,5,5,5]\n",
        "\n",
        "# Calculate observed agreement po\n",
        "agreements = sum(np.array(ratings1) == np.array(ratings2))\n",
        "total = len(ratings1)\n",
        "p_o = agreements / total\n",
        "\n",
        "# Calculate expected agreement pe\n",
        "rater1_counts = Counter(ratings1)\n",
        "rater2_counts = Counter(ratings2)\n",
        "\n",
        "n = sum(rater1_counts.values())  # total ratings\n",
        "pe_categories = 0\n",
        "for category in set(ratings1 + ratings2):\n",
        "    pe_categories += (rater1_counts[category] / n) * (rater2_counts[category] / n)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Observed Agreement (p_o): {p_o:.2f}\")\n",
        "print(f\"Expected Agreement (p_e): {pe_categories:.2f}\")"
      ],
      "metadata": {
        "id": "Ugpsa6ISoTkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef83fdb-cb4c-426c-f76b-cd306d0ba6b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observed Agreement (p_o): 0.50\n",
            "Expected Agreement (p_e): 0.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Authenticity - 2 classes\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "ratings1 = [5,5,5,5,5,5,5,5,5,5,5,3,5,3,5,5]\n",
        "ratings2 = [5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5]\n",
        "\n",
        "# Calculate observed agreement po\n",
        "agreements = sum(np.array(ratings1) == np.array(ratings2))\n",
        "total = len(ratings1)\n",
        "p_o = agreements / total\n",
        "\n",
        "# Calculate expected agreement pe\n",
        "rater1_counts = Counter(ratings1)\n",
        "rater2_counts = Counter(ratings2)\n",
        "\n",
        "n = sum(rater1_counts.values())  # total ratings\n",
        "pe_categories = 0\n",
        "for category in set(ratings1 + ratings2):\n",
        "    pe_categories += (rater1_counts[category] / n) * (rater2_counts[category] / n)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Observed Agreement (p_o): {p_o:.2f}\")\n",
        "print(f\"Expected Agreement (p_e): {pe_categories:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFXXR80Og3I0",
        "outputId": "e535ed23-38a0-4886-bb76-10226ec7e59a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observed Agreement (p_o): 0.88\n",
            "Expected Agreement (p_e): 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbnMBoUhiPK8",
        "outputId": "afb13b0c-c7e4-4e61-ec7c-1c8970d0e101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observed Agreement (p_o): 0.69\n",
            "Expected Agreement (p_e): 0.44\n"
          ]
        }
      ],
      "source": [
        "#Fluency - 5 classes\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "ratings1 = [4,5,5,4,5,5,5,5,5,5,3,4,5,4,5,4]\n",
        "ratings2 = [4,5,5,4,5,4,5,5,4,5,3,4,4,3,5,5]\n",
        "\n",
        "# Calculate observed agreement po\n",
        "agreements = sum(np.array(ratings1) == np.array(ratings2))\n",
        "total = len(ratings1)\n",
        "p_o = agreements / total\n",
        "\n",
        "# Calculate expected agreement pe\n",
        "rater1_counts = Counter(ratings1)\n",
        "rater2_counts = Counter(ratings2)\n",
        "\n",
        "n = sum(rater1_counts.values())  # total ratings\n",
        "pe_categories = 0\n",
        "for category in set(ratings1 + ratings2):\n",
        "    pe_categories += (rater1_counts[category] / n) * (rater2_counts[category] / n)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Observed Agreement (p_o): {p_o:.2f}\")\n",
        "print(f\"Expected Agreement (p_e): {pe_categories:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fluency - 2 classes\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "ratings1 = [5,5,5,5,5,5,5,5,5,5,3,5,5,5,5,5]\n",
        "ratings2 = [5,5,5,5,5,5,5,5,5,5,3,5,5,3,5,5]\n",
        "\n",
        "# Calculate observed agreement po\n",
        "agreements = sum(np.array(ratings1) == np.array(ratings2))\n",
        "total = len(ratings1)\n",
        "p_o = agreements / total\n",
        "\n",
        "# Calculate expected agreement pe\n",
        "rater1_counts = Counter(ratings1)\n",
        "rater2_counts = Counter(ratings2)\n",
        "\n",
        "n = sum(rater1_counts.values())  # total ratings\n",
        "pe_categories = 0\n",
        "for category in set(ratings1 + ratings2):\n",
        "    pe_categories += (rater1_counts[category] / n) * (rater2_counts[category] / n)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Observed Agreement (p_o): {p_o:.2f}\")\n",
        "print(f\"Expected Agreement (p_e): {pe_categories:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_vAPXvDg_o8",
        "outputId": "5c65cc0c-c901-4b64-b55f-b266b0935bcb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observed Agreement (p_o): 0.94\n",
            "Expected Agreement (p_e): 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def calculate_cohens_kappa(ratings1, ratings2):\n",
        "    \"\"\"\n",
        "    Calculate Cohen's Kappa for two arrays of categorical ratings.\n",
        "\n",
        "    Args:\n",
        "    ratings1 (list or array): Ratings by Annotator 1.\n",
        "    ratings2 (list or array): Ratings by Annotator 2.\n",
        "\n",
        "    Returns:\n",
        "    float: Cohen's Kappa coefficient.\n",
        "    \"\"\"\n",
        "    kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    return kappa\n",
        "\n",
        "# Example usage:\n",
        "ratings1 = [4,5,5,5,5,4,5,4,4,4,4,3,5,3,4,5]\n",
        "ratings2 = [5,5,5,5,5,5,4,5,5,4,4,5,5,5,5,5]\n",
        "\n",
        "\n",
        "\n",
        "kappa = calculate_cohens_kappa(ratings1, ratings2)\n",
        "print(f\"Cohen's Kappa: {kappa:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgEShxCmzpbK",
        "outputId": "d261300d-5f9c-4783-867b-319771fe76ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa: 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def calculate_cohens_kappa(ratings1, ratings2):\n",
        "    \"\"\"\n",
        "    Calculate Cohen's Kappa for two arrays of categorical ratings.\n",
        "\n",
        "    Args:\n",
        "    ratings1 (list or array): Ratings by Annotator 1.\n",
        "    ratings2 (list or array): Ratings by Annotator 2.\n",
        "\n",
        "    Returns:\n",
        "    float: Cohen's Kappa coefficient.\n",
        "    \"\"\"\n",
        "    kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    return kappa\n",
        "\n",
        "# Example usage:\n",
        "ratings1 = [4,5,5,4,5,5,5,5,5,5,3,4,5,4,5,4]\n",
        "ratings2 = [4,5,5,4,5,4,5,5,4,5,3,4,4,3,5,5]\n",
        "\n",
        "\n",
        "\n",
        "kappa = calculate_cohens_kappa(ratings1, ratings2)\n",
        "print(f\"Cohen's Kappa: {kappa:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0LOEzKXzjt7",
        "outputId": "8ed735b3-5909-4302-b6d8-76e9c7cc7824"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa: 0.44\n"
          ]
        }
      ]
    }
  ]
}